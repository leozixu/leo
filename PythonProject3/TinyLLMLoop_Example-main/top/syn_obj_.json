{"response": "```cuda#include <algorithm>\n#include <cuda_bf16.h>\n#include <cuda_fp16.h>\n#include <cuda_fp8.h>\n#include <cuda_runtime.h>\n#include <float.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define INT4(value) (reinterpret_cast<int4 *>(&(value))[0])\n#define FLOAT4(value) (reinterpret_cast<float4 *>(&(value))[0])\n#define HALF2(value) (reinterpret_cast<half2 *>(&(value))[0])\n#define BFLOAT2(value) (reinterpret_cast<__nv_bfloat162 *>(&(value))[0])\n#define LDST64BITS(value) (reinterpret_cast<float2 *>(&(value))[0])\n#define LDST128BITS(value) (reinterpret_cast<float4 *>(&(value))[0])\n\n// FP16\n// HGEMM naive: compute one c[i,j]\n// element per threads, all row major\n__global__ void hgemm_f16_kernel(half *a, half *b, half *c, int M, int N,\n                                       int K) {\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  int m = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (m < M && n < N) {\n    half psum = 0.0;\n#pragma unroll\n    for (int k = 0; k < K; k++) {\n      // m row in a matrix, n col in b matrix\n      psum += a[m * K + k] * b[k * N + n];\n    }\n    c[m * N + n] = psum; // c[m,n]\n  }\n}\n\n#include <torch/extension.h>\n#include <torch/types.h>\n#define STRINGFY(str) #str\n#define TORCH_BINDING_COMMON_EXTENSION(func)                                   \\\n  m.def(STRINGFY(func), &func, STRINGFY(func));\n\n#define CHECK_TORCH_TENSOR_DTYPE(T, th_type)                                   \\\n  if (((T).options().dtype() != (th_type))) {                                  \\\n    std::cout << \"Tensor Info:\" << (T).options() << std::endl;                 \\\n    throw std::runtime_error(\"values must be \" #th_type);                      \\\n  }\n\n#define CHECK_TORCH_TENSOR_SHAPE(T, S0, S1)                                    \\\n  if (((T).size(0) != (S0)) || ((T).size(1) != (S1))) {                        \\\n    throw std::runtime_error(\"Tensor size mismatch!\");                         \\\n  }\n\n// HGEMM naive: compute one c[i,j] element per threads, all row major\nvoid hgemm_llm_kernel(torch::Tensor a, torch::Tensor b, torch::Tensor c) {\n  CHECK_TORCH_TENSOR_DTYPE(a, torch::kHalf)\n  CHECK_TORCH_TENSOR_DTYPE(b, torch::kHalf)\n  CHECK_TORCH_TENSOR_DTYPE(c, torch::kHalf)\n  const int M = a.size(0);\n  const int K = a.size(1);\n  const int N = b.size(1);\n  CHECK_TORCH_TENSOR_SHAPE(a, M, K)\n  CHECK_TORCH_TENSOR_SHAPE(b, K, N)\n  CHECK_TORCH_TENSOR_SHAPE(c, M, N)\n  constexpr int BM = 32;\n  constexpr intt BN = 32;\n\n  dim3 block(BN, BM);\n  dim3 grid((N + BN - 1) / BN, (M + BM - 1) / BM);\n\n  hgemm_f16_kernel<<<grid, block>>>(\n      reinterpret_cast<half *>(a.data_ptr()),\n      reinterpret_cast<half *>(b.data_ptr()),\n      reinterpret_cast<half *>(c.data_ptr()), M, N, K);\n}\n```"}